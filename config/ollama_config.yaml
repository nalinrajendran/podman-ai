# Ollama Configuration for podman_ai.py
# This file customizes the Ollama server and podman_ai settings for the Podman AI assistant.
# Place this file in the same directory as podman_ai.py or update the CONFIG_FILE path in the script.
# Use YAML syntax and refer to comments for guidance.

# Ollama server settings
ollama:
  # URL of the Ollama server
  # Example: http://localhost:11434/api/generate (default for local Ollama)
  # Change if running Ollama on a different host or port (e.g., http://192.168.1.100:11434/api/generate)
  url: "http://localhost:11434/api/generate"

  # Model to use for generating Podman commands
  # Example: llama3.2 (default), mistral, or any model installed in Ollama
  # Check available models with `ollama list`
  model: "podman-ai"

# podman_ai settings
podman_ai:
  # List of risky Podman subcommands that require user confirmation
  # Example: Commands like 'rm' (remove container), 'rmi' (remove image), etc.
  # Add or remove subcommands based on your needs
  risky_commands:
    - "rm"        # Remove containers
    - "rmi"       # Remove images
    - "stop"      # Stop containers
    - "kill"      # Kill containers
    - "prune"     # Prune unused resources
    - "uninstall" # Uninstall Podman components